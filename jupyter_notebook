{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2ded081",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "In the following parts, you will create a bigram and trigram model to classify if documents are written by humans\n",
    "or by ChatGPT. Download the datafile humvgpt.zip1. This file contains around 40, 000 human written and 20, 000\n",
    "ChatGPT written documents. The data is stored in separate text files named hum.txt and gpt.txt respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4691d13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from collections import defaultdict\n",
    "import string\n",
    "import re\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9bfe1f",
   "metadata": {},
   "source": [
    "a) Clean the data by removing all punctuation except “,.?!” and converting all words to lower case. You\n",
    "may also find it helpful to add special <START> and <END> tokens to each document for calculating\n",
    "N-gram probabilities. Partition the initial 90% of the data to a training set and final 10% to test set (the\n",
    "data has already been shuffled for you)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2f4f0ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Function to pre-process data\n",
    "def pre_process_file(file):\n",
    "    processed_documents = []\n",
    "\n",
    "    with open(file, 'r+') as f:\n",
    "\n",
    "        documents = f.read().splitlines()\n",
    "\n",
    "        # Remove punctuation except “,.?!”\n",
    "        punctuation_to_remove = ''.join(set(string.punctuation) - set(',.?!'))\n",
    "        translation_table = str.maketrans('', '', punctuation_to_remove)\n",
    "\n",
    "        for doc in documents:\n",
    "            # remove punctuation and make all lower case\n",
    "            modified_doc = doc.translate(translation_table).lower()\n",
    "\n",
    "            # Add <START> before and <END> after the modified content\n",
    "            processed_doc = '<START> ' + modified_doc.strip() + ' <END>'\n",
    "            processed_documents.append(processed_doc)\n",
    "        \n",
    "    return processed_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "121a76a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "filepath_gpt = '/Users/anouckrietveld/Documents/Columbia/ML for DS/gpt.txt'\n",
    "filepath_hum = '/Users/anouckrietveld/Documents/Columbia/ML for DS/hum.txt'\n",
    "\n",
    "# Data pre-processing\n",
    "file_gpt = pre_process_file(filepath_gpt)\n",
    "file_hum = pre_process_file(filepath_hum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c699488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition the data into training and test sets\n",
    "def train_test_split(file):\n",
    "    train_size = int(0.9 * len(file))\n",
    "    train_set = file[:train_size]\n",
    "    test_set = file[train_size:]\n",
    "    return train_set, test_set\n",
    "\n",
    "gpt_train_set, gpt_test_set = train_test_split(file_gpt)\n",
    "hum_train_set, hum_test_set = train_test_split(file_hum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335dc6e5",
   "metadata": {},
   "source": [
    "(b) Train a bigram and trigram model by calculating the N-gram frequencies per class in the training corpus.\n",
    "Calculate and report the percentage of bigrams/trigrams in the test set (counted with repeats) that do not\n",
    "appear in the training corpus (this is called the OOV rate). You should calculate two OOV rates, one for\n",
    "bigrams and one for trigrams."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf75d5f",
   "metadata": {},
   "source": [
    "First, we are going to calculate the N-gram frequencies per class in the training corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf9f23f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ngram_frequency(file, n):\n",
    "    ngram_freq = defaultdict(int)\n",
    "    \n",
    "    for document in file:\n",
    "        words = document.split()\n",
    "        for i in range(n - 1, len(words)):\n",
    "            ngram = tuple(words[i - n + 1:i + 1])\n",
    "            ngram_freq[ngram] += 1\n",
    "    \n",
    "    return ngram_freq\n",
    "\n",
    "## Bigram\n",
    "# Class gpt\n",
    "train_bigram_frequency_gpt = calculate_ngram_frequency(gpt_train_set, 2)\n",
    "\n",
    "# Class hum\n",
    "train_bigram_frequency_hum = calculate_ngram_frequency(hum_train_set, 2)\n",
    "\n",
    "## Trigram\n",
    "# Class gpt\n",
    "train_trigram_frequency_gpt = calculate_ngram_frequency(gpt_train_set, 3)\n",
    "\n",
    "# Class hum\n",
    "train_trigram_frequency_hum = calculate_ngram_frequency(hum_train_set, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5309d0c8",
   "metadata": {},
   "source": [
    "Now, we are going through the bigram and trigram frequencies of the test sets and count the number of times that the bigrams or trigrams are not present in the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4345594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and report the OOV rate\n",
    "\n",
    "# Calculate the bigram frequencies for the test sets\n",
    "test_bigram_frequency_gpt = calculate_ngram_frequency(gpt_test_set, 2)\n",
    "test_bigram_frequency_hum = calculate_ngram_frequency(hum_test_set, 2)\n",
    "\n",
    "# Calculate the trigram frequencies for the test sets\n",
    "test_trigram_frequency_gpt = calculate_ngram_frequency(gpt_test_set, 3)\n",
    "test_trigram_frequency_hum = calculate_ngram_frequency(hum_test_set, 3)\n",
    "\n",
    "# Merge the two default dicts together and sum their values\n",
    "def merge_and_sum(dict1, dict2):\n",
    "    merged_dict = defaultdict(int)\n",
    "    # Add values from dict1\n",
    "    for key, value in dict1.items():\n",
    "        merged_dict[key] += value\n",
    "    # Add values from dict2\n",
    "    for key, value in dict2.items():\n",
    "        merged_dict[key] += value\n",
    "    return merged_dict\n",
    "\n",
    "# Bigram train and test\n",
    "train_bigram_frequency = merge_and_sum(train_bigram_frequency_gpt, train_bigram_frequency_hum)\n",
    "test_bigram_frequency = merge_and_sum(test_bigram_frequency_gpt, test_bigram_frequency_hum)\n",
    "\n",
    "# Trigram train and test\n",
    "train_trigram_frequency = merge_and_sum(train_trigram_frequency_gpt, train_trigram_frequency_hum)\n",
    "test_trigram_frequency = merge_and_sum(test_trigram_frequency_gpt, test_trigram_frequency_hum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c527eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oov_rate(test, train):\n",
    "    oov_count = 0\n",
    "    total_bigrams_test = sum(test.values())\n",
    "\n",
    "    for bigram, freq in test.items():\n",
    "        if not bigram in train.keys():\n",
    "            oov_count += freq # counted with repeats\n",
    "    oov_rate = (oov_count / total_bigrams_test) * 100\n",
    "    \n",
    "    return oov_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3470bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OVV rate of the bigram is 9.64%\n",
      "The OVV rate of the trigram is 35.27%\n"
     ]
    }
   ],
   "source": [
    "# Calculate bigram frequency\n",
    "oov_rate_bigram = oov_rate(test_bigram_frequency, train_bigram_frequency)\n",
    "print(f'The OVV rate of the bigram is {round(oov_rate_bigram,2)}%')\n",
    "\n",
    "# Calculate trigram frequency\n",
    "oov_rate_trigram = oov_rate(test_trigram_frequency, train_trigram_frequency)\n",
    "print(f'The OVV rate of the trigram is {round(oov_rate_trigram,2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c731b1d3",
   "metadata": {},
   "source": [
    "(c) Evaluate the two models on the test set and report the classification accuracy. Which model performs better\n",
    "and why? Your justification should consider the bigram and trigram OOV rate. This study will also tell you how difficult or easy it is to distinguish human vs. AI generated text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f62ab50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate |V|, the number of unique words in the document\n",
    "\n",
    "def count_unique_words(documents):\n",
    "    # Concatenate all documents into a single string\n",
    "    all_text = ' '.join(documents)\n",
    "\n",
    "    # Tokenize the string into individual words\n",
    "    words = all_text.split()\n",
    "\n",
    "    # Use a set to store unique words\n",
    "    unique_words = set(words)\n",
    "\n",
    "    # Count the number of unique words\n",
    "    num_unique_words = len(unique_words)\n",
    "\n",
    "    return num_unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3536689",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Incorrect n_minus_1_gram_count Calculation for Trigrams: In the trigram version of laplacian_smoothing, \n",
    "## you must adjust the denominator to use the frequency of the two-word sequence preceding the third word in the \n",
    "## trigram, not just the first word. This requires changing how n_minus_1_gram_freq is accessed. For trigrams, it's not about \n",
    "## a single word (ngram[0]), but about the two-word sequence (ngram[:-1]).\n",
    "\n",
    "def laplacian_smoothing(ngram, n_minus_1_gram_freq, ngram_freq, vocab_size, alpha=1):\n",
    "    \"\"\"\n",
    "    Calculate the smoothed probability of a bigram given the class.\n",
    "    \n",
    "    Parameters:\n",
    "    - bigram: The current bigram tuple (word_i, word_i+1).\n",
    "    - unigram_freq: Frequency dictionary of unigrams for the class.\n",
    "    - bigram_freq: Frequency dictionary of bigrams for the class.\n",
    "    - vocab_size: The size of the vocabulary (|V|).\n",
    "    - alpha: Smoothing parameter (default is 1 for add-one smoothing).\n",
    "    \n",
    "    Returns:\n",
    "    - The smoothed probability of the bigram.\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(ngram) == 2:  # For bigrams, the context is the first word\n",
    "        context = ngram[:-1]  # This is equivalent to (ngram[0],)\n",
    "    else:  # For trigrams (and potentially higher n-grams), the context is the first two words\n",
    "        context = ngram[:-1]  # This slices everything but the last element, suitable for trigrams and adaptable for higher n-grams\n",
    "    \n",
    "    ngram_count = ngram_freq.get(ngram, 0) + alpha\n",
    "    n_minus_1_gram_count = n_minus_1_gram_freq.get(context, 0) + vocab_size * alpha\n",
    "    smoothed_probability = ngram_count / n_minus_1_gram_count\n",
    "    \n",
    "    return smoothed_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99adf190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_given_document(test_set_dict, n_minus1_gram_frequency_gpt, ngram_frequency_gpt, n_minus1_gram_frequency_hum, ngram_frequency_hum, V, P_gpt, P_hum, n):\n",
    "    document_probabilities = {}\n",
    "    \n",
    "    for document, true_label in test_set_dict.items():\n",
    "        ngram_prob_gpt = 0\n",
    "        ngram_prob_hum = 0\n",
    "        \n",
    "        # Split the document into tokens\n",
    "        words = document.split()\n",
    "\n",
    "        # Adjust the range to handle trigrams if n=3\n",
    "        for i in range(len(words) - (n - 1)):\n",
    "            # Adjust to create either bigram or trigram based on n\n",
    "            ngram = tuple(words[i:i+n])\n",
    "            \n",
    "            # Calculate probability of the ngram given the class (human or GPT) using Laplacian smoothing\n",
    "            # gpt\n",
    "            laplacian_gpt = laplacian_smoothing(ngram, n_minus1_gram_frequency_gpt, ngram_frequency_gpt, abs_V, alpha=1)\n",
    "            ngram_prob_gpt += math.log(laplacian_gpt)\n",
    "    \n",
    "            # hum\n",
    "            laplacian_hum = laplacian_smoothing(ngram, n_minus1_gram_frequency_hum, ngram_frequency_hum, abs_V, alpha=1)  \n",
    "            ngram_prob_hum += math.log(laplacian_hum)\n",
    "\n",
    "        # Add the log class prior probabilities\n",
    "        prob_gpt_given_document = ngram_prob_gpt + math.log(P_gpt)\n",
    "        prob_hum_given_document = ngram_prob_hum + math.log(P_hum)\n",
    "        \n",
    "        # Store the calculated probabilities for each document\n",
    "        document_probabilities[document] = {'GPT': prob_gpt_given_document, 'Human': prob_hum_given_document}\n",
    "    \n",
    "    return document_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc138262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(probabilities_file, test_set_dictionary):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = len(probabilities_file)\n",
    "    \n",
    "    for document, probabilities in probabilities_file.items():\n",
    "        \n",
    "        # Determine the predicted label based on which probability is higher\n",
    "        predicted_label = 'gpt' if probabilities['GPT'] > probabilities['Human'] else 'hum'\n",
    "        \n",
    "        # Get the true label from the test_set_dictionary\n",
    "        true_label = test_set_dictionary[document]\n",
    "        \n",
    "        # Compare the predicted label to the true label\n",
    "        if predicted_label == true_label:\n",
    "            correct_predictions += 1\n",
    "    \n",
    "    # Calculate the accuracy\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a10427c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the test set of hum and gpt together and add true_label to the dictionary\n",
    "\n",
    "# Merge lists\n",
    "test_set = gpt_test_set + hum_test_set\n",
    "\n",
    "# Create dictionary where document:true_label\n",
    "test_set_dict = {doc: 'gpt' if doc in gpt_test_set else 'hum' for doc in test_set}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60d8a093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the absolute V value, which is the set of all unique words across all documents in the entire corpus\n",
    "corpus = file_gpt + file_hum\n",
    "abs_V = count_unique_words(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4071e05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate P(y) for every class\n",
    "prob_gpt = len(file_gpt) / (len(file_gpt)+len(file_hum))\n",
    "prob_hum = len(file_hum) / (len(file_gpt)+len(file_hum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66625965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the probability for gpt and hum given the document\n",
    "\n",
    "# Calculate unigram frequencies gpt and hum\n",
    "train_unigram_frequency_gpt = calculate_ngram_frequency(gpt_train_set, 1)\n",
    "train_unigram_frequency_hum = calculate_ngram_frequency(hum_train_set, 1)\n",
    "\n",
    "bigram_file_probabilities = prob_given_document(test_set_dict, train_unigram_frequency_gpt, train_bigram_frequency_gpt, train_unigram_frequency_hum, train_bigram_frequency_hum, abs_V, prob_gpt, prob_hum, 2)\n",
    "\n",
    "trigram_file_probabilities = prob_given_document(test_set_dict, train_bigram_frequency_gpt, train_trigram_frequency_gpt, train_bigram_frequency_hum, train_trigram_frequency_hum, abs_V, prob_gpt, prob_hum, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5712ba8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classification accuracy for the bigram model is: 95.86%\n",
      "The classification accuracy for the trigram model is: 94.33%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy\n",
    "bigram_accuracy = calculate_accuracy(bigram_file_probabilities, test_set_dict)\n",
    "print(f\"The classification accuracy for the bigram model is: {100*bigram_accuracy:.2f}%\")\n",
    "\n",
    "trigram_accuracy = calculate_accuracy(trigram_file_probabilities, test_set_dict)\n",
    "print(f\"The classification accuracy for the trigram model is: {100*trigram_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f860b703",
   "metadata": {},
   "source": [
    "**Bigram Model Accuracy**: The bigram model has higher accuracy compared to the trigram model. This could be because bigrams are simpler and more frequent in language, making it easier for the model to learn from the training data. Additionally, the OOV rate for bigrams is lower, indicating that a significant portion of the test set's bigrams were present in the training data, facilitating better classification.\n",
    "\n",
    "**Trigram Model Accuracy**: The trigram model has lower accuracy, possibly due to the higher complexity of trigrams and a higher OOV rate. Trigrams are less frequent and may capture more nuanced patterns in the text, which might not generalize well to unseen data. The higher OOV rate for trigrams suggests that a larger portion of the test set's trigrams were not present in the training data, leading to difficulties in classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0979743b",
   "metadata": {},
   "source": [
    "(iii) (a) Using T = 50, generate 5 sentences with 20 words each from the human and ChatGPT bigrams/trigrams\n",
    "(you should have 20 sentences total using T = 50). Which text corpus and N-gram model generates\n",
    "the best sentences? What happens when you increase or decrease the temperature? You should begin\n",
    "generation with N − 1 <START> tokens and stop once you generate the <END> token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d50f1100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(word_counts, temperature):\n",
    "    \"\"\"Apply softmax to a list of frequencies with a temperature parameter.\"\"\"\n",
    "    \n",
    "    # Extract just the frequencies from word_counts\n",
    "    frequencies = [count for _, count in word_counts]\n",
    "    \n",
    "    # Apply the temperature scaling on the frequencies\n",
    "    exp_frequencies = np.exp(np.array(frequencies) / temperature)\n",
    "    \n",
    "    # Normalize the exponentiated frequencies to get probabilities\n",
    "    sum_exp_frequencies = np.sum(exp_frequencies)\n",
    "    probabilities = exp_frequencies / sum_exp_frequencies\n",
    "    \n",
    "    # Pair each word with its corresponding probability\n",
    "    next_word_probabilities = [(word, prob) for (word, _), prob in zip(word_counts, probabilities)]\n",
    "    \n",
    "    return next_word_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f363838a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_word_probabilities(context, ngram_freq, temperature):\n",
    "    \"\"\"Given the context, return a dictionary of next word probabilities, with non-qualifying ngrams set to 0.\"\"\"\n",
    "    \n",
    "    # Initialize all possible next words with a frequency of 0\n",
    "    candidates = {bigram: 0 for bigram in ngram_freq} \n",
    "    \n",
    "    # Update the frequencies for ngrams that match the context\n",
    "    for ngram, count in ngram_freq.items():\n",
    "        if ngram[:-1] == context:\n",
    "            candidates[ngram] = count\n",
    "     \n",
    "    word_counts = [(key[-1], value) for key, value in candidates.items()]\n",
    "    \n",
    "    # Now candidates have all possible words with their respective frequencies, zero if they don't follow the context\n",
    "    probabilities = softmax(word_counts, temperature)\n",
    "    \n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91487949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(ngram_freq, temperature, n, max_length=20):\n",
    "    \"\"\"Generate a sentence using the N-gram frequencies with temperature scaling.\"\"\"\n",
    "    \n",
    "    sentence = ['<START>'] * (n - 1)  # For bigram, this will be just ['<START>']\n",
    "    \n",
    "    while len(sentence) < max_length:\n",
    "        context = tuple(sentence[-(n - 1):])\n",
    "        \n",
    "        word_probabilities = get_next_word_probabilities(context, ngram_freq, temperature)\n",
    "        \n",
    "        words, probs = zip(*word_probabilities)  # Unpack the list of tuples\n",
    "        next_word = np.random.choice(words, p=probs)\n",
    "        \n",
    "        sentence.append(next_word)\n",
    "        \n",
    "        if next_word == '<END>':\n",
    "            break\n",
    "            \n",
    "    return ' '.join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c9fbba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentences(ngram_freq, temperature, n, max_length, num_sentences):\n",
    "    \n",
    "    sentences = []\n",
    "    for _ in range(num_sentences):\n",
    "        sentence = generate_sentence(ngram_freq, temperature, n, max_length)\n",
    "        sentences.append(sentence)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5761f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating sentences for: Bigram - GPT\n",
      "<START> can be a good tractor rgb social rises for the same they are a good and the united states\n",
      "<START> the same amounts members the same that the same small management by the same reliable hit finds that the\n",
      "<START> services at bright hiding bulbs out snow a few wifi scanner provinces sump 505 healthy to the same daylight\n",
      "<START> . <END>\n",
      "<START> shopping managing is a good enough crowdfunding . <END>\n",
      "\n",
      "\n",
      "Generating sentences for: Bigram - Human\n",
      "<START> the same tanks and the same of the same much gets an moldy reaction lag let long goes founding\n",
      "<START> the same it s to the same aca mines tie one so as a lot of the same from\n",
      "<START> the same businesses and the same , and the same arbitrary them systems affecting and the same . <END>\n",
      "<START> the same the same john criteria irritates that the same abbreviated rap mb gave ? gender farmland arbiter they\n",
      "<START> the same from the same this is a lot of the same can be bans bulb the same nothing\n",
      "\n",
      "\n",
      "Generating sentences for: Trigram - GPT\n",
      "<START> <START> certain causing agreement , surface being may the . . . by its people 12 , the when\n",
      "<START> <START> , remember more by affected you and time people . of so need investment impaired radio to .\n",
      "<START> <START> underwater be other place vietnam have what because and is various are temporary to to growls this what\n",
      "<START> <START> that on schools a many layer due it is important to . on has unexpected gasoline . .\n",
      "<START> <START> its their in in . of of change platform stock to bodies divisions sales home considered what by\n",
      "\n",
      "\n",
      "Generating sentences for: Trigram - Human\n",
      "<START> <START> what to time spicy if proved of to interest doing enjoy register . . . . . .\n",
      "<START> <START> medical . deformed solution a that locked was default a . a to your , gear to s\n",
      "<START> <START> <END>\n",
      "<START> <START> eventually surfaces by i these <END>\n",
      "<START> <START> different agree this the financial rate the , that the of bigger impulse you react tasks revise if\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_generated_sentences(ngram_freq, temperature, n, max_length, num_sentences, description):\n",
    "    \"\"\"\n",
    "    Generate and print sentences based on the n-gram frequencies.\n",
    "    \n",
    "    Parameters:\n",
    "    - ngram_freq: Dictionary of n-gram frequencies\n",
    "    - temperature: Temperature parameter for softmax\n",
    "    - n: The n in n-gram (2 for bigram, 3 for trigram)\n",
    "    - max_length: Maximum length of generated sentences\n",
    "    - num_sentences: Number of sentences to generate\n",
    "    - description: Description of the source and n-gram model\n",
    "    \"\"\"\n",
    "    print(f\"Generating sentences for: {description}\")\n",
    "    sentences = generate_sentences(ngram_freq, temperature, n, max_length, num_sentences)\n",
    "    for sentence in sentences:\n",
    "        print(sentence)\n",
    "    print(\"\\n\")  # Print a newline for better separation between groups\n",
    "\n",
    "# Now, we can call this function for each of your cases:\n",
    "temperature = 50\n",
    "\n",
    "# Bigram - GPT\n",
    "print_generated_sentences(test_bigram_frequency_gpt, 20, 2, 20, 5, \"Bigram - GPT\")\n",
    "\n",
    "# Bigram - Human\n",
    "print_generated_sentences(test_bigram_frequency_hum, 20, 2, 20, 5, \"Bigram - Human\")\n",
    "\n",
    "# Trigram - GPT\n",
    "print_generated_sentences(test_trigram_frequency_gpt, 20, 3, 20, 5, \"Trigram - GPT\")\n",
    "\n",
    "# Trigram - Human\n",
    "print_generated_sentences(test_trigram_frequency_hum, 20, 3, 20, 5, \"Trigram - Human\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3c71de",
   "metadata": {},
   "source": [
    "**Decreasing Temperature**: Lowering T results in more deterministic and coherent sentences.\n",
    "This reduction favors the generation of grammatically correct sequences, though they may lack\n",
    "creativity.\n",
    "**Increasing Temperature**: Enhancing T further amplifies randomness, potentially leading to\n",
    "nonsensical and highly unpredictable sentence structures\n",
    "\n",
    "**Bigram Models** are likely to generate more grammatically coherent sentences at lower tem-\n",
    "peratures, though such sentences might be less interesting due to their simplicity.\n",
    "**Trigram Models**, with their capacity to understand more context, are better suited for gener-\n",
    "ating complex and nuanced sentences. However, the balance between coherence and creativity\n",
    "is crucial, especially as the temperature increases.\n",
    "\n",
    "Also, refer to my written assignment for a more comprehensive answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda591ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
